{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install and Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install necessary packages\n",
    "\n",
    "# %pip install keras_vggface\n",
    "# %pip install keras_applications\n",
    "# %pip install opencv-python\n",
    "# %pip install tensorflow\n",
    "# %pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import torch as torch\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing import image\n",
    "from keras_vggface import utils\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "\n",
    "import keras.utils as image\n",
    "\n",
    "data_path =  '../Data/' # This will have to vary based on who is running the code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training labels and noun list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Angela', 'Dwight', 'Jim', 'Kevin', 'Michael', 'Pam']\n",
      "['a photo of a window', 'a photo of a water', 'a photo of a university', 'a photo of a tv', 'a photo of a trophy', 'a photo of a tree', 'a photo of a train', 'a photo of a Toilet', 'a photo of a theatre', 'a photo of a television', 'a photo of a telephone', 'a photo of a table', 'a photo of a street', 'a photo of a stapler', 'a photo of a snow', 'a photo of a sea', 'a photo of a road', 'a photo of a railway', 'a photo of a proposal', 'a photo of a phone', 'a photo of a party', 'a photo of a park', 'a photo of a paper', 'a photo of a office', 'a photo of a night', 'a photo of a newspaper', 'a photo of a museum', 'a photo of a mug', 'a photo of a money', 'a photo of a mirror', 'a photo of a milk', 'a photo of a manager', 'a photo of a machine', 'a photo of a lunch', 'a photo of a library', 'a photo of a kitchen', 'a photo of a house', 'a photo of a hotel', 'a photo of a hospital', 'a photo of a group', 'a photo of a garden', 'a photo of a game', 'a photo of a gallery', 'a photo of a fruit', 'a photo of a football', 'a photo of a food', 'a photo of a fish', 'a photo of a fire', 'a photo of a drink', 'a photo of a door', 'a photo of a dog', 'a photo of a desk', 'a photo of a cup', 'a photo of a Copier', 'a photo of a computer', 'a photo of a company', 'a photo of a coffee', 'a photo of a church', 'a photo of a Chair', 'a photo of a cat', 'a photo of a cash', 'a photo of a car', 'a photo of a cabinet', 'a photo of a bridge', 'a photo of a bread', 'a photo of a bottle', 'a photo of a book', 'a photo of a boat', 'a photo of a bedroom', 'a photo of a bed', 'a photo of a bar', 'a photo of a baby', 'a photo of a art', 'a photo of a aircraft', 'a photo of a agriculture']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load the training labels\n",
    "face_label_filename = data_path + 'face-labels.pickle'\n",
    "with open(face_label_filename, \"rb\") as \\\n",
    "    f: class_dictionary = pickle.load(f)\n",
    "\n",
    "class_list = [value for _, value in class_dictionary.items()]\n",
    "print(class_list)\n",
    "\n",
    "# load list of 73 nouns \n",
    "import csv\n",
    "\n",
    "with open(data_path + 'nouns.csv', newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    data = list(reader)\n",
    "\n",
    "data = [line[0] for line in data]\n",
    "\n",
    "nouns = ['a photo of a ' + s for s in data]\n",
    "\n",
    "print(nouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "# pretrained clip model and processor for object detection\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "print('CLIP model loaded')\n",
    "# custom transfer model for detecting characters\n",
    "face_model = load_model(\n",
    "    'transfer_learning_trained' +\n",
    "    '_the_office_cnn_model.h5')\n",
    "\n",
    "# pretrained model to detect faces\n",
    "facecascade =  cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect faces and objects in the test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'torch' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n torch ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# for detecting faces\n",
    "facecascade =  cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# change path if needed\n",
    "test_path = '../Data/Test_Images'\n",
    "\n",
    "for root, _, files in os.walk(test_path):\n",
    "    for file in files:\n",
    "\n",
    "        if file.endswith(\"png\") or file.endswith(\"jpg\") or file.endswith(\"jpeg\"):\n",
    "        # path of the image\n",
    "        # print(path)\n",
    "            path = os.path.join(root, file)\n",
    "            print(path)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "\n",
    "\n",
    "        label = os.path.basename(root).replace(\" \", \".\").lower()\n",
    "\n",
    "        # load the image\n",
    "        imgtest = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "        image_array = np.array(imgtest, \"uint8\")\n",
    "\n",
    "        # get the faces detected in the image\n",
    "        all_faces = facecascade.detectMultiScale(imgtest, \n",
    "            scaleFactor=1.1, minNeighbors=5)\n",
    "\n",
    "        # loop through each face and get each character\n",
    "        character_vector = []\n",
    "        for i in range(len(all_faces)):\n",
    "            faces = [all_faces[i]]\n",
    "            for (x_, y_, w, h) in faces:\n",
    "                # draw the face detected\n",
    "                face_detect = cv2.rectangle(\n",
    "                    imgtest, (x_, y_), (x_+w, y_+h), (255, 0, 255), 2)\n",
    "\n",
    "                # resize the detected face to 224x224\n",
    "                size = (image_width, image_height)\n",
    "                roi = image_array[y_: y_ + h, x_: x_ + w]\n",
    "                resized_image = cv2.resize(roi, size)\n",
    "\n",
    "                # prepare the image for prediction\n",
    "                x = image.img_to_array(resized_image)\n",
    "                x = np.expand_dims(x, axis=0)\n",
    "                x = utils.preprocess_input(x, version=1)\n",
    "\n",
    "                # making prediction\n",
    "                predicted_prob = face_model.predict(x)\n",
    "                character_vector += [class_list[predicted_prob[0].argmax()]]\n",
    "                \n",
    "        inputs = processor(text=nouns, images=imgtest, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "        outputs = clip_model(**inputs)\n",
    "        logits_per_image = outputs.logits_per_image # this is the image-text similarity score\n",
    "        probs = logits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilitie\n",
    "        #print(probs)\n",
    "        #ind = torch.topk(probs.flatten(), 3).indices\n",
    "        ind = probs.squeeze().argsort()[-5:]\n",
    "        sentence = [data[k] for k in ind]\n",
    "        plt.imshow(cv2.cvtColor(face_detect, cv2.COLOR_BGR2RGB))\n",
    "        plt.show()\n",
    "        print(character_vector)\n",
    "        print(sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb79b4760a3db271a0094282eed73d55c4f5e3b07e2a097a4c69ffa68186c117"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
